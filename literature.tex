\chapter{Literature review}\label{chapt:lit}
The idea of using satellite data has been researched in many communities. Several automated and semi-automated ways are proposed to tackle the problem of detecting roads. These ways are quite different due to the different strategies and algorithms used by them. \Cref{sec:cnn,sec:resNet,sec:encoderDecoder} documents the various techniques used in this report. \Cref{sec:roadDetection,sec:sr} paves a path on how did the automation of road-extraction come into existence.

\section{Convolutional Neural Networks~(CNN)}\label{sec:cnn}
`\textit{A CNN is a type of Artificial Neural Network~(ANN) where convolutional blocks are used instead of basic 1-D multilayer perceptrons.}'

For understanding CNN, we need to see what an Artificial Neural Network~(ANN) is. ANN is a system of interconnected neurons used to model complex functions for classification and regression. Any model is made up of at least three layers: An input layer, one or more hidden layers, and an output layer. The most basic ANN model consists of multilayer perceptrons. \Cref{subsection:types_of_layers} shows different types of layers that can be used for a convolutional neural network. Several non-linear activation functions such as rectified linear unit (ReLU)or sigmoid, or even custom functions can be applied at any point on the layers to produce activation maps. These activation maps finally help us in classification or segmentation.

\subsection{Types of layers}
\label{subsection:types_of_layers}
The general structure of any convolutional neural network can be categorized as follows:
\begin{enumerate}[(a)]
    \item \textbf{Input layer}:
        The input layer is the input of any neural network. The input of computer vision problems is usually an image that is basically a matrix of numbers stacked multiple times~(channels). An RGB image has 3 channels(\cref{fig:RGB_image}).
        \begin{figure}[h!]
            \centering
            \includegraphics[width=0.8\textwidth]{rgb_image_as_array}
            \caption[RGB image shown as matrix]{RGB image shown as matrix. Resolution of this image is $5\times5$.}
            \label{fig:RGB_image}
        \end{figure}
        
    \item \textbf{Convolution layer}:
        As the name suggests, this is the most important layer of convolutional neural networks. A convolutional layer contains a set of filters~(or kernel or node) whose parameters need to be learned. The height and width of the filter are predefined when defining the layer and are smaller than those of the input volume. For most images, the dimension of the filter is taken $3\times3$ or $5\times5$, but if the given image is large, filters up to size $11\times11$ can also be used. One catch is `a filter is not two dimensional'. It takes in the dimension of the input it is provided with.

        Applying a filter to an image means taking the inner product of the image and the filter within the overlapping area, which keeps on moving. Suppose the dimension of an image is $W\times W$ and dimension of the filter is $k\times k$, the dimension of the output image will be $(W-k+1)\times (W-k+1)$. Once a filter has been applied, it passes through an activation function, and the resulting image is called the output of that filter.
    
        The formula for convolutional neural network is: \begin{equation} a_{i,j} = f(\sum_{m=0}^2\sum_{n=0}^2 W_{m,n}X_{m+i,n+j}) \end{equation} where $\text{X}_{i, j}$ represents the pixel value of line $i$ in column $j$ of the image and $W_{m, n}$ represents the weight of column $n$ in row $m$. Similarly, $a_{i,j}$ represents the value of column $j$ of the $i$ row of the feature map; the activation function is represented by $f$. (Example: \Cref{fig:convolution_filter_process}).
        \begin{figure}[h!]
            \begin{subfigure}[b]{0.35\textwidth}
            \includegraphics[width=\textwidth]{cnn_filter_input}
            \caption{}
            \end{subfigure}~
            \begin{subfigure}[b]{0.2\textwidth}
            \includegraphics[width=\textwidth]{cnn_filter}
            \caption{}
            \end{subfigure}~
            \begin{subfigure}[b]{0.2\textwidth}
            \includegraphics[width=\textwidth]{cnn_filter_convolution}
            \caption{}
            \end{subfigure}~
            \begin{subfigure}[b]{0.2\textwidth}
            \includegraphics[width=\textwidth]{cnn_filter_output}
            \caption{}
            \end{subfigure}
            \caption[The process of convolution]{The process of convolution: \textbf{(a)} Input vector. \textbf{(b)} Filter being convoluted on (a). \textbf{(c)} Result of (b) convoluting over (a). \textbf{(d)} Final result after applying activation function $a_{i,j}>2$.}
            \label{fig:convolution_filter_process}
        \end{figure}

        Each filter in the convolution layer uses the output of the previous layer to connect and calculate. The number of filters is predefined and usually kept in powers of 2. Suppose a layer has 64 filters of dimension $3\times3$, the input image has dimensions of $1000\times1000$, the resulting output will be of dimension $998\times998\times64$. If this is passed into another layer consisting of 128 filters and the same dimension, the new output will be $996\times996\times128$.

    \item \textbf{Pooling layer}:
        The amount of data after the multilayer convolution layer is huge. To reduce the time taken for calculations, we need to reduce the size of the matrix. This means we need to reduce the number of parameters of the fully-connected layer. This is done by using a pooling layer. The pooling layer finds the image's eigenvalues, which are then used as the basis of classification. The pooling layer also prevents data from overfitting.

        The pooled layer function similar to data resampling. The filter propagates in the same way as the filters in the convolution layer. The difference between the two is that instead of taking the inner product with the filter, we take the maximum or average value of the window. The most used pooling methods are max pooling, mean pooling, and random pooling layers. In a max-pooling layer, the maximum value of a pre-specified window replaces the given dataset. If the dataset is replaced by averaging the contents in the window, it is called an average pooling layer. A number is selected randomly according to the probability matrix in random pooling.

    \item \textbf{Fully connected layer}:
        Unlike the filters in the convolution layer where only some pixels are taken into account, each fully connected layer uses the previous layer's global information. This means each nodeâ€™s calculation is connected with the weights of all nodes in the previous layer. This is the reason this layer is usually used at the end of the model. 

        Because it takes in the global context, this means the complete information received from convolution and pooling operations is taken into consideration for output from this layer. Given the outputs, an activation function is applied for classification purposes.
\end{enumerate}

\section*{There are some special types of convolutional layers which perform specific tasks:}
\subsection{Dilated convolution}
\label{sec:Dilated_Convolution}
\textit{Convolution with dilated filter; Also called atrous convolution.}

    Dilated convolutional layers are used in image segmentation, where its class labels each pixel. As the name suggests, dilated convolution is basically dilating the filter before doing the usual convolution. Dilating the filter means expanding, taking in distant elements instead of the adjacent elements in the input matrix to calculate weights. These filters are also called upsampled filters. The empty positions in between are filled with zeros, thus making this kind of sparse matrix. The distance in a dilated convolution filter is determined by the dilation coefficient D~(also called dilation rate). If the dilation rate is 1, it means it is the standard convolutional kernel. If the dilation rate is 2, then there is a skip of one pixel per input. In general, if there is a dilation rate of $n$, skip of $n-1$ pixels per input.
    
    \Cref{fig:dilated_layer} shows how the kernel elements are matched to input elements in a D-dilated $3\times3$ convolution. Also, notice how the dilated convolution layer has increased the spatial context~(a.k.a feature maps or receptive field or the field of view) without decreasing the resolution of the output.
    \begin{figure}[h!]
        \centering
        \includegraphics[width=0.5\textwidth]{cnn_dilated_layer}
        \caption[Dilated convolution layer]{Notice how dilated convolution layer increases the capture of spatial context. Dilation rate is 2 in this figure.}
        \label{fig:dilated_layer}
    \end{figure}


\subsection{Strided convolution}
    In a regular convolution, we usually shift the windows by 1 pixel. However, for some large images, it is necessary to shrink feature maps to make computations feasible. In stride convolution, we change the length of stride in each step. This means we skip some input values while performing convolution operations and thus decreasing the output dimension. In general, this operation does a trad off between resource consumption and information retrieval.

\section[Residual Networks]{Residual Networks}\label{sec:resNet}

In a neural network, weights receive an update proportional to the gradient of current weight in each iteration. \begin{equation} \text{Change\ In\ Weight}_{i} \propto \frac{\partial(\text{Error\ Function)}}{\partial(\text{weight}_{i})} \end{equation} When the gradient of the training model becomes vanishingly small, the change in weight becomes negligible, and thus model training essentially stops. This problem is called the problem of vanishing gradient.

Conventionally, as the CNNs become deeper, the problems related to vanishing gradients became severe. To solve this problem, {Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun}\cite{ResNet} developed the concept of residual networks~(also called ResNet or skip connections). Residual neural networks tackle vanishing gradients by skipping connections or making shortcuts to jump over some layers (\cref{fig:residual_network}). Typical ResNet models are implemented with double or triple layer skips that contain non-linearities (ReLU) and batch normalization in between.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.3\textwidth]{lit_resnet}
    \caption[A residual network]{Canonical form of a residual neural network. Layer-2 is skipped over activation from Layer-1.}
    \label{fig:residual_network}
\end{figure}

While training, an additional matrix can be used to train the skip connections as well. These are usually identity matrices and not so hard on memory; hence they can be used to quickly train much deeper networks than what was achieved with the conventional neural networks. ResNet also increases performance while training the model. This is due to the face that some layers are essentially skipped, and the gradients that were earlier vanishing now converge faster.

Many improvements to the original ResNet architecture have been made since its inception. Pre-trained models like ResNet (34, 50, 101) are already available in popular libraries like Tensorflow and PyTorch. ResNet is considered one of the most significant innovations in the area of neural networks.

\section{Encoder-Decoder}\label{sec:encoderDecoder}
The encoder-decoder architecture is the standard neural machine translation method that rivals and, in some cases, outperforms classical statistical machine translation methods.

Though this architecture is very new, having only been pioneered in 2014. The encoder-decoder architecture has become a modern approach for sequence-to-sequence (seq2seq) predictions. Road detection needs a sequence-to-sequence learning framework to keep track of spatial characteristics. It has three components: an encoder network, a decoder network, and an intermediate network.

The encoders are trained with the decoders. There are no labels (hence unsupervised). The loss function is based on computing the delta between the actual and reconstructed input. The optimizer will try to train both encoder and decoder so that the features that matter the most should have lower reconstruction loss. This also means that, given an encoded sequence, we do not try to reconstruct the actual input, but rather try to map inputs to specific outputs. For example, given a map, it maps roads to true and assigns everything else to false. This is how a lossy compression algorithm helps us in the segmentation of an image.

\section{Road detection}\label{sec:roadDetection}
The journey to digitally identify the roads started with the topic of reducing cost and increasing productivity. One of the first road detection algorithms were static methods and focused on the color and geometry of roads. These methods were quite unreliable, mainly because of the discontinuities arising due to the similar color range of roads and shadows~\cite{Detecting_interections_using_color,Detecting_roads_using_color}. Another disadvantage was the need for high-resolution images (ca 0.2~m~-~0.5~m), which is hard to get. The idea was then improvised to include texture cues for road detection~\cite{using_texture_for_road_detection,baumgartner1999automatic}. The accuracy was indeed better, but the need for a very high-resolution image remained a problem. With the machine learning techniques, the pixel-level classification started to give improved performance~\cite{road_detection_using_neural_nets_SVM,road_detection_using_env_learning,road_detection_using_SVM_online_learning}.

The Support Vector Machine~(SVM) methods have a good generalization ability and thus are widely used in object detection from a given image. They are, at most times, more accurate and give consistent robust results than other classification techniques such as K-nearest neighbor. Nevertheless, the estimation of kernel functions and the choice of the dimensional space and training samples led to the SVM classifier~\cite{YagerSowmya2003,melgani2004classification}. This was done using edge-based features such as gradient, intensity, edge length, and classified objects in a high-resolution image. SVM approaches are well suited for multispectral data, where we have an adequate number of vectors to classify objects~\cite{Simler2011}.

In 2003, Tu-Ko presented a robust approach, in which a back-propagation~(BP) neural network was trained with the spectral and edge information to find the road centerline. Although errors prop in, the model was still useful as a whole. Disadvantages of BP neural network include slow convergence speed, the need for an extensive training data set, inability to find global minima, and over-fitting risks. Since then, many types of neural networks like fuzzy neural networks~\cite{mokhtarzade2008automatic} and Convolutional Neural Networks~(CNN) have been used to capture the spatial contexts of road networks, giving much better accuracies on fewer resources.

With the developments of Graphics Processing Units~(GPUs) and Tensor Processing Units~(TPUs) and the progress of ResNet architectures, even Deep Convolutional Neural Networks~(DCNN) have been researched and changed the way road extraction works. The recent developments with encoder-decoders, dilations, and fuzzy logic have inspired state-of-the-art algorithms like UNet, SegNet, LinkNet, and its further improvements like D-LinkNet and AD-LinkNet~(\cref{fig:compare_road}). Moreover, competitions pitch in the researchers by providing them with a precise dataset and motivating them to thrive and excel in the world race.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{compare_road}
  \caption[Comparison of different road-detection models]{Comparison of different road-detection models. Adapted from~\cite{AD-LinkNet}.}
  \label{fig:compare_road}
\end{figure}


\section{Super Resolution~(SR)}\label{sec:sr}
Many models have proposed methods to obtain a super-resolution image from the given image. Most of the techniques use bicubic upsampling of the image as an input to the proposed neural network model. However, this was inefficient as a significant part of memory is required for performing bicubic upsampling. The authors of~\cite{EDSR} additionally questioned the architectural optimality used in the existing networks. Though neural networks provide significantly improved performance in terms of peak signal-to-noise ratio (PSNR) in the SR problem, it is also proven to be quite sensitive to minor architectural changes. The outputs differ in performance with different training techniques. The authors further proposed an enhanced deep super-resolution network (EDSR) with performance exceeding the past state-of-the-art super-resolution models. This was achieved by carefully designing model architecture using sophisticated optimization methods in training the neural networks.

Secondly, the authors noticed that most existing models treat the super-resolution of different scale factors as an independent problem. There is no consideration of learning the mutual relationship between different scales. With efforts such as the development of Very Deep Super Resolution~(VSDR), this relationship is proved to be feasible and much more accurate than the single-scale models, though at the cost of heavier computation time and memory utilization. Further work was restricted by a lack of ability to use very deep neural networks.

With the break-through changes brought by concepts of skip-connects and residual blocks, neural networks were shown to be trained with 100+ layers. SRNet solved incorporated the ResNet architecture and solved the problem of high memory utilization. Because ResNet architecture was proposed to solve higher-level computer vision problems such as image classification and detection, its use directly for a low-level problem like computer vision can be optimized.

EDSR proposes using a modified type of residual block to remove the unnecessary elements and optimize the model's performance. Secondly, a new architecture is used with fewer parameters but showing comparable performance. \Cref{fig:compare_SR} shows a comparison between different SR algorithms.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{compare_SR}
  \caption[Comparison of different super-resolution models]{Comparison of different super-resolution models. HR is raw high resolution image taken. Adapted from~\cite{EDSR}.}
  \label{fig:compare_SR}
\end{figure}
